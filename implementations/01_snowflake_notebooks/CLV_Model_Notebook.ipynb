{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End CLV Model Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Snowflake Feature Store\n",
    "We will use the Use-Case to show how Snowflake Feature Store (and Model Registry) can be used to maintain & store features, retrieve them for training and perform micro-batch inference.\n",
    "\n",
    "In the development (TRAINING) enviroment we will \n",
    "- create FeatureViews in the Feature Store that maintain the required customer-behaviour features.\n",
    "- use these Features to train a model, and save the model in the Snowflake model-registry.\n",
    "- plot the clusters for the trained model to visually verify. \n",
    "\n",
    "In the production (SERVING) environment we will\n",
    "- re-create the FeatureViews on production data\n",
    "- generate an Inference FeatureView that uses the saved model to perform incremental inference\n",
    "\n",
    "# Feature Engineering & Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import os\n",
    "import json\n",
    "import timeit\n",
    "\n",
    "# SNOWFLAKE\n",
    "# Snowpark\n",
    "from snowflake.snowpark import Session, DataFrame, Window, WindowSpec\n",
    "\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake.snowpark import Session, Row\n",
    "\n",
    "# Snowflake Feature Store\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureView,\n",
    "    Entity\n",
    ")\n",
    "\n",
    "# COMMON FUNCTIONS\n",
    "from helper.useful_fns import dataset_check_and_update, formatSQL, create_ModelRegistry, create_FeatureStore, create_SF_Session, get_spine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Snowflake connection and database parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You might have more than one threads sharing the Session object trying to update sql_simplifier_enabled. Updating this while other tasks are running can potentially cause unexpected behavior. Please update the session configuration before starting the threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connection Established with the following parameters:\n",
      "User                        : JARCHEN\n",
      "Role                        : \"ACCOUNTADMIN\"\n",
      "Database                    : \"RETAIL_REGRESSION_DEMO\"\n",
      "Schema                      : \"DS\"\n",
      "Warehouse                   : \"RETAIL_REGRESSION_DEMO_WH\"\n",
      "Snowflake version           : 9.39.2\n",
      "Snowpark for Python version : 1.38.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "role_name, database_name, schema_name, session, warehouse_name = create_SF_Session(\n",
    "    connection_file = '../../connection.json',\n",
    "    role=\"ACCOUNTADMIN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='CLV_MODEL_POOL_CPU already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create compute pool\n",
    "def create_compute_pool(name: str, instance_family: str, min_nodes: int = 1, max_nodes: int = 10) -> list[Row]:\n",
    "    query = f\"\"\"\n",
    "        CREATE COMPUTE POOL IF NOT EXISTS {name}\n",
    "            MIN_NODES = {min_nodes}\n",
    "            MAX_NODES = {max_nodes}\n",
    "            INSTANCE_FAMILY = {instance_family}\n",
    "    \"\"\"\n",
    "    return session.sql(query).collect()\n",
    "\n",
    "compute_pool = \"CLV_MODEL_POOL_CPU\"\n",
    "create_compute_pool(compute_pool, \"CPU_X64_L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL DEVELOPMENT\n",
    "* Create Snowflake Model-Registry\n",
    "* Create Snowflake Feature-Store\n",
    "* Establish and Create CUSTOMER Entity in the development Snowflake FeatureStore\n",
    "* Create Source Data references and perform basic data-cleansing\n",
    "* Create & Run Preprocessing Function to create features\n",
    "* Create FeatureView_Preprocess from Preprocess Dataframe SQL\n",
    "* Create training data from FeatureView_Preprocess (asof join)\n",
    "* Create & Fit Snowpark-ml pipeline \n",
    "* Save model in Model Registry\n",
    "* 'Verify' and approve model\n",
    "* Create new FeatureView_Model_Inference with Transforms UDF + KMeans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Registry (_MODELLING) already exists\n",
      "Feature Store (_FEATURE_STORE) already exists\n"
     ]
    }
   ],
   "source": [
    "# Create/Reference Snowflake Model Registry - Common across Environments\n",
    "mr = create_ModelRegistry(session, database_name, '_MODELLING')\n",
    "\n",
    "# Create/Reference Snowflake Feature Store - Common across Environments\n",
    "fs = create_FeatureStore(session, database_name, '_FEATURE_STORE', warehouse_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETAIL_REGRESSION_DEMO.DS.CUSTOMERS 10000\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"AGE\"  |\"ANNUAL_INCOME\"  |\"LOYALTY_TIER\"  |\"GENDER\"  |\"STATE\"  |\"TENURE_MONTHS\"  |\"SIGNUP_DATE\"  |\"CREATED_AT\"                |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|1              |36     |66091.00         |medium          |female    |WA       |6                |2025-06-19     |2025-12-19 03:18:43.893000  |\n",
      "|2              |32     |47309.00         |low             |female    |NSW      |5                |2025-07-19     |2025-12-19 03:18:43.893000  |\n",
      "|3              |18     |54797.00         |low             |male      |NSW      |29               |2023-07-19     |2025-12-19 03:18:43.893000  |\n",
      "|4              |35     |27852.00         |low             |female    |NSW      |22               |2024-02-19     |2025-12-19 03:18:43.893000  |\n",
      "|5              |28     |60224.00         |low             |male      |VIC      |48               |2021-12-19     |2025-12-19 03:18:43.893000  |\n",
      "|6              |38     |71674.00         |low             |male      |QLD      |57               |2021-03-19     |2025-12-19 03:18:43.893000  |\n",
      "|7              |18     |84957.00         |low             |female    |WA       |9                |2025-03-19     |2025-12-19 03:18:43.893000  |\n",
      "|8              |34     |28399.00         |high            |male      |TAS      |54               |2021-06-19     |2025-12-19 03:18:43.893000  |\n",
      "|9              |20     |89406.00         |high            |female    |NSW      |21               |2024-03-19     |2025-12-19 03:18:43.893000  |\n",
      "|10             |30     |57351.00         |medium          |female    |QLD      |37               |2022-11-19     |2025-12-19 03:18:43.893000  |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_tbl = '.'.join([database_name, schema_name,'CUSTOMERS'])\n",
    "cust_sdf = session.table(cust_tbl)\n",
    "print(cust_tbl, cust_sdf.count())\n",
    "cust_sdf.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUSTOMER Entity\n",
    "Establish and Create CUSTOMER Entity in Snowflake FeatureStore for this Use-Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "|\"NAME\"    |\"JOIN_KEYS\"      |\"DESC\"                          |\"OWNER\"       |\n",
      "------------------------------------------------------------------------------\n",
      "|CUSTOMER  |[\"CUSTOMER_ID\"]  |Primary Key for CUSTOMER ORDER  |ACCOUNTADMIN  |\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if \"CUSTOMER\" not in json.loads(fs.list_entities().select(F.to_json(F.array_agg(\"NAME\", True))).collect()[0][0]):\n",
    "    customer_entity = Entity(name=\"CUSTOMER\", join_keys=[\"CUSTOMER_ID\"],desc=\"Primary Key for CUSTOMER ORDER\")\n",
    "    fs.register_entity(customer_entity)\n",
    "else:\n",
    "    customer_entity = fs.get_entity(\"CUSTOMER\")\n",
    "\n",
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Create & Load Source Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Feature engineering pipelines are defined using Snowpark dataframes (or SQL expressions).  In the `QS_feature_engineering_fns.py` file we have created two feature engineering functions to create our pipeline :\n",
    "* __uc01_load_data__(order_data: DataFrame, lineitem_data: DataFrame, order_returns_data: DataFrame) -> DataFrame   \n",
    "* __uc01_pre_process__(data: DataFrame) -> DataFrame\n",
    "\n",
    "`uc01_load_data`, takes the source tables, as dataframe objects, and joins them together, performing some data-cleansing by replacing NA's with default values. It returns a dataframe as it's output.\n",
    "\n",
    "`uc01_pre_process`, takes the dataframe output from `uc01_load_data`  and performs aggregation on it to derive some features that will be used in our segmentation model.  It returns a dataframe as output, which we will use to provide the feature-pipeline definition within our FeatureView.\n",
    "\n",
    "In this way we can build up a complex pipeline step-by-step and use it to derive a FeatureView, that will be maintained as a pipeline in Snowflake.\n",
    "\n",
    "We will import the functions, and create dataframes from them using the dataframes we created earlier pointing to the tables in our TRAINING (Development) schema.  We will use the last dataframe we create at the end of the pipeline as our input to the FeatureView.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Functions\n",
    "from feature_engineering_fns import uc01_load_data, uc01_pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TABLE ROW_COUNTS IN DS\n",
      "RETAIL_REGRESSION_DEMO.DS.CUSTOMERS 10000\n",
      "<snowflake.snowpark.table.Table object at 0x11860b130> 10000\n"
     ]
    }
   ],
   "source": [
    "# Tables\n",
    "cust_tbl                    = '.'.join([database_name, schema_name,'CUSTOMERS'])\n",
    "behavior_tbl                = '.'.join([database_name, schema_name,'PURCHASE_BEHAVIOR'])\n",
    "\n",
    "# Snowpark Dataframe\n",
    "cust_sdf              = session.table(cust_tbl)\n",
    "behavior_tbl          = session.table(behavior_tbl)\n",
    "\n",
    "# Row Counts\n",
    "print(f'''\\nTABLE ROW_COUNTS IN {schema_name}''')\n",
    "print(cust_tbl, cust_sdf.count())\n",
    "print(behavior_tbl, behavior_tbl.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "raw_data = uc01_load_data(cust_sdf, behavior_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH SNOWPARK_LEFT AS (\n",
      "  SELECT\n",
      "    \"CUSTOMER_ID\" AS \"l_0000_CUSTOMER_ID\",\n",
      "    \"AGE\" AS \"AGE\",\n",
      "    \"ANNUAL_INCOME\" AS \"ANNUAL_INCOME\",\n",
      "    \"LOYALTY_TIER\" AS \"LOYALTY_TIER\",\n",
      "    \"GENDER\" AS \"GENDER\",\n",
      "    \"STATE\" AS \"STATE\",\n",
      "    \"TENURE_MONTHS\" AS \"TENURE_MONTHS\",\n",
      "    \"SIGNUP_DATE\" AS \"SIGNUP_DATE\",\n",
      "    \"CREATED_AT\" AS \"CREATED_AT\"\n",
      "  FROM RETAIL_REGRESSION_DEMO.DS.CUSTOMERS\n",
      "), SNOWPARK_RIGHT AS (\n",
      "  SELECT\n",
      "    \"CUSTOMER_ID\" AS \"r_0001_CUSTOMER_ID\",\n",
      "    \"AVG_ORDER_VALUE\" AS \"AVG_ORDER_VALUE\",\n",
      "    \"PURCHASE_FREQUENCY\" AS \"PURCHASE_FREQUENCY\",\n",
      "    \"RETURN_RATE\" AS \"RETURN_RATE\",\n",
      "    \"LIFETIME_VALUE\" AS \"LIFETIME_VALUE\",\n",
      "    \"LAST_PURCHASE_DATE\" AS \"LAST_PURCHASE_DATE\",\n",
      "    \"TOTAL_ORDERS\" AS \"TOTAL_ORDERS\",\n",
      "    \"UPDATED_AT\" AS \"UPDATED_AT\"\n",
      "  FROM RETAIL_REGRESSION_DEMO.DS.PURCHASE_BEHAVIOR\n",
      "), cte AS (\n",
      "  SELECT\n",
      "    *\n",
      "  FROM (\n",
      "    SNOWPARK_LEFT AS SNOWPARK_LEFT\n",
      "      LEFT OUTER JOIN SNOWPARK_RIGHT AS SNOWPARK_RIGHT\n",
      "        ON (\n",
      "          \"l_0000_CUSTOMER_ID\" = \"r_0001_CUSTOMER_ID\"\n",
      "        )\n",
      "  )\n",
      "), cte_2 AS (\n",
      "  SELECT\n",
      "    *\n",
      "    RENAME (\"CREATED_AT\" AS CUSTOMER_CREATED_AT, \"l_0000_CUSTOMER_ID\" AS CUSTOMER_ID, \"UPDATED_AT\" AS BEHAVIOR_UPDATED_AT)\n",
      "  FROM cte AS cte\n",
      ")\n",
      "SELECT\n",
      "  \"CUSTOMER_ID\",\n",
      "  \"AGE\",\n",
      "  \"ANNUAL_INCOME\",\n",
      "  \"LOYALTY_TIER\",\n",
      "  \"GENDER\",\n",
      "  \"STATE\",\n",
      "  \"TENURE_MONTHS\",\n",
      "  \"SIGNUP_DATE\",\n",
      "  \"CUSTOMER_CREATED_AT\",\n",
      "  \"AVG_ORDER_VALUE\",\n",
      "  \"PURCHASE_FREQUENCY\",\n",
      "  \"RETURN_RATE\",\n",
      "  \"LIFETIME_VALUE\",\n",
      "  \"LAST_PURCHASE_DATE\",\n",
      "  \"TOTAL_ORDERS\",\n",
      "  \"BEHAVIOR_UPDATED_AT\"\n",
      "FROM cte_2 AS cte_2\n"
     ]
    }
   ],
   "source": [
    "# Format and print the SQL for the Snowpark Dataframe\n",
    "rd_sql = formatSQL(raw_data.queries['queries'][0], True)\n",
    "print(os.linesep.join(rd_sql.split(os.linesep)[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"AGE\"  |\"ANNUAL_INCOME\"  |\"LOYALTY_TIER\"  |\"GENDER\"  |\"STATE\"  |\"TENURE_MONTHS\"  |\"SIGNUP_DATE\"  |\"CUSTOMER_CREATED_AT\"       |\"AVG_ORDER_VALUE\"  |\"PURCHASE_FREQUENCY\"  |\"RETURN_RATE\"  |\"LIFETIME_VALUE\"  |\"LAST_PURCHASE_DATE\"  |\"TOTAL_ORDERS\"  |\"BEHAVIOR_UPDATED_AT\"       |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|1              |36     |66091.00         |medium          |female    |WA       |6                |2025-06-19     |2025-12-19 03:18:43.893000  |177.22             |4.07                  |16.00          |4696.96           |2025-12-03            |24              |2025-12-18 03:18:45.088000  |\n",
      "|2              |32     |47309.00         |low             |female    |NSW      |5                |2025-07-19     |2025-12-19 03:18:43.893000  |109.46             |1.00                  |7.00           |201.86            |2025-11-05            |5               |2025-12-04 04:53:05         |\n",
      "|3              |18     |54797.00         |low             |male      |NSW      |29               |2023-07-19     |2025-12-19 03:18:43.893000  |68.96              |1.44                  |16.00          |2849.53           |2025-11-24            |42              |2025-12-18 03:18:45.088000  |\n",
      "|4              |35     |27852.00         |low             |female    |NSW      |22               |2024-02-19     |2025-12-19 03:18:43.893000  |150.57             |1.58                  |7.00           |5408.44           |2025-11-22            |35              |2025-12-18 03:18:45.088000  |\n",
      "|5              |28     |60224.00         |low             |male      |VIC      |48               |2021-12-19     |2025-12-19 03:18:43.893000  |76.04              |1.67                  |4.00           |7551.00           |2025-11-27            |80              |2025-12-18 03:18:45.088000  |\n",
      "|6              |38     |71674.00         |low             |male      |QLD      |57               |2021-03-19     |2025-12-19 03:18:43.893000  |124.33             |1.72                  |12.00          |13241.65          |2025-12-02            |98              |2025-12-09 01:34:58         |\n",
      "|7              |18     |84957.00         |low             |female    |WA       |9                |2025-03-19     |2025-12-19 03:18:43.893000  |102.99             |1.92                  |1.00           |2333.36           |2025-11-20            |17              |2025-12-18 03:18:45.088000  |\n",
      "|8              |34     |28399.00         |high            |male      |TAS      |54               |2021-06-19     |2025-12-19 03:18:43.893000  |149.68             |3.10                  |9.00           |31342.35          |2025-12-05            |167             |2025-12-18 03:18:45.088000  |\n",
      "|9              |20     |89406.00         |high            |female    |NSW      |21               |2024-03-19     |2025-12-19 03:18:43.893000  |123.88             |2.93                  |12.00          |10059.19          |2025-12-08            |62              |2025-12-13 21:05:53         |\n",
      "|10             |30     |57351.00         |medium          |female    |QLD      |37               |2022-11-19     |2025-12-19 03:18:43.893000  |158.47             |3.12                  |0.00           |24755.53          |2025-11-26            |115             |2025-12-18 03:18:45.088000  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & Run Preprocessing Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "preprocessed_data = uc01_pre_process(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"AGE\"  |\"LOYALTY_TIER\"  |\"GENDER\"  |\"STATE\"  |\"TENURE_MONTHS\"  |\"SIGNUP_DATE\"  |\"CUSTOMER_CREATED_AT\"       |\"AVG_ORDER_VALUE\"  |\"PURCHASE_FREQUENCY\"  |\"RETURN_RATE\"  |\"LIFETIME_VALUE\"  |\"LAST_PURCHASE_DATE\"  |\"TOTAL_ORDERS\"  |\"BEHAVIOR_UPDATED_AT\"       |\"ANNUAL_INCOME\"  |\"AVERAGE_ORDER_PER_MONTH\"  |\"DAYS_SINCE_LAST_PURCHASE\"  |\"DAYS_SINCE_SIGNUP\"  |\"EXPECTED_DAYS_BETWEEN_PURCHASES\"  |\"DAYS_SINCE_EXPECTED_LAST_PURCHASE_DATE\"  |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|1              |36     |medium          |female    |WA       |6                |2025-06-19     |2025-12-19 03:18:43.893000  |177.22             |4.07                  |16.00          |4696.96           |2025-12-03            |24              |2025-12-18 03:18:45.088000  |66091            |4.000000                   |26                          |193                  |7.371007                           |19                                        |\n",
      "|2              |32     |low             |female    |NSW      |5                |2025-07-19     |2025-12-19 03:18:43.893000  |109.46             |1.00                  |7.00           |201.86            |2025-11-05            |5               |2025-12-04 04:53:05         |47309            |1.000000                   |54                          |163                  |30.000000                          |24                                        |\n",
      "|3              |18     |low             |male      |NSW      |29               |2023-07-19     |2025-12-19 03:18:43.893000  |68.96              |1.44                  |16.00          |2849.53           |2025-11-24            |42              |2025-12-18 03:18:45.088000  |54797            |1.448276                   |35                          |894                  |20.833333                          |14                                        |\n",
      "|4              |35     |low             |female    |NSW      |22               |2024-02-19     |2025-12-19 03:18:43.893000  |150.57             |1.58                  |7.00           |5408.44           |2025-11-22            |35              |2025-12-18 03:18:45.088000  |27852            |1.590909                   |37                          |679                  |18.987342                          |18                                        |\n",
      "|5              |28     |low             |male      |VIC      |48               |2021-12-19     |2025-12-19 03:18:43.893000  |76.04              |1.67                  |4.00           |7551.00           |2025-11-27            |80              |2025-12-18 03:18:45.088000  |60224            |1.666667                   |32                          |1471                 |17.964072                          |14                                        |\n",
      "|6              |38     |low             |male      |QLD      |57               |2021-03-19     |2025-12-19 03:18:43.893000  |124.33             |1.72                  |12.00          |13241.65          |2025-12-02            |98              |2025-12-09 01:34:58         |71674            |1.719298                   |27                          |1746                 |17.441860                          |10                                        |\n",
      "|7              |18     |low             |female    |WA       |9                |2025-03-19     |2025-12-19 03:18:43.893000  |102.99             |1.92                  |1.00           |2333.36           |2025-11-20            |17              |2025-12-18 03:18:45.088000  |84957            |1.888889                   |39                          |285                  |15.625000                          |23                                        |\n",
      "|8              |34     |high            |male      |TAS      |54               |2021-06-19     |2025-12-19 03:18:43.893000  |149.68             |3.10                  |9.00           |31342.35          |2025-12-05            |167             |2025-12-18 03:18:45.088000  |28399            |3.092593                   |24                          |1654                 |9.677419                           |14                                        |\n",
      "|9              |20     |high            |female    |NSW      |21               |2024-03-19     |2025-12-19 03:18:43.893000  |123.88             |2.93                  |12.00          |10059.19          |2025-12-08            |62              |2025-12-13 21:05:53         |89406            |2.952381                   |21                          |650                  |10.238908                          |11                                        |\n",
      "|10             |30     |medium          |female    |QLD      |37               |2022-11-19     |2025-12-19 03:18:43.893000  |158.47             |3.12                  |0.00           |24755.53          |2025-11-26            |115             |2025-12-18 03:18:45.088000  |57351            |3.108108                   |33                          |1136                 |9.615385                           |23                                        |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH SNOWPARK_LEFT AS (\n",
      "  SELECT\n",
      "    \"CUSTOMER_ID\" AS \"l_0000_CUSTOMER_ID\",\n",
      "    \"AGE\" AS \"AGE\",\n",
      "    \"ANNUAL_INCOME\" AS \"ANNUAL_INCOME\",\n",
      "    \"LOYALTY_TIER\" AS \"LOYALTY_TIER\",\n",
      "    \"GENDER\" AS \"GENDER\",\n",
      "    \"STATE\" AS \"STATE\",\n",
      "    \"TENURE_MONTHS\" AS \"TENURE_MONTHS\",\n",
      "    \"SIGNUP_DATE\" AS \"SIGNUP_DATE\",\n",
      "    \"CREATED_AT\" AS \"CREATED_AT\"\n",
      "  FROM RETAIL_REGRESSION_DEMO.DS.CUSTOMERS\n",
      "), SNOWPARK_RIGHT AS (\n",
      "  SELECT\n",
      "    \"CUSTOMER_ID\" AS \"r_0001_CUSTOMER_ID\",\n",
      "    \"AVG_ORDER_VALUE\" AS \"AVG_ORDER_VALUE\",\n",
      "    \"PURCHASE_FREQUENCY\" AS \"PURCHASE_FREQUENCY\",\n",
      "    \"RETURN_RATE\" AS \"RETURN_RATE\",\n",
      "    \"LIFETIME_VALUE\" AS \"LIFETIME_VALUE\",\n",
      "    \"LAST_PURCHASE_DATE\" AS \"LAST_PURCHASE_DATE\",\n",
      "    \"TOTAL_ORDERS\" AS \"TOTAL_ORDERS\",\n",
      "    \"UPDATED_AT\" AS \"UPDATED_AT\"\n",
      "  FROM RETAIL_REGRESSION_DEMO.DS.PURCHASE_BEHAVIOR\n",
      "), cte AS (\n",
      "  SELECT\n",
      "    *\n",
      "  FROM (\n",
      "    SNOWPARK_LEFT AS SNOWPARK_LEFT\n",
      "      LEFT OUTER JOIN SNOWPARK_RIGHT AS SNOWPARK_RIGHT\n",
      "        ON (\n",
      "          \"l_0000_CUSTOMER_ID\" = \"r_0001_CUSTOMER_ID\"\n",
      "        )\n",
      "  )\n",
      "), cte_2 AS (\n",
      "  SELECT\n",
      "    *\n",
      "    RENAME (\"CREATED_AT\" AS CUSTOMER_CREATED_AT, \"l_0000_CUSTOMER_ID\" AS CUSTOMER_ID, \"UPDATED_AT\" AS BEHAVIOR_UPDATED_AT)\n",
      "  FROM cte AS cte\n",
      ")\n",
      "SELECT\n",
      "  \"CUSTOMER_ID\",\n",
      "  \"AGE\",\n",
      "  \"LOYALTY_TIER\",\n",
      "  \"GENDER\",\n",
      "  \"STATE\",\n",
      "  \"TENURE_MONTHS\",\n",
      "  \"SIGNUP_DATE\",\n",
      "  \"CUSTOMER_CREATED_AT\",\n",
      "  \"AVG_ORDER_VALUE\",\n",
      "  \"PURCHASE_FREQUENCY\",\n",
      "  \"RETURN_RATE\",\n",
      "  \"LIFETIME_VALUE\",\n",
      "  \"LAST_PURCHASE_DATE\",\n",
      "  \"TOTAL_ORDERS\",\n",
      "  \"BEHAVIOR_UPDATED_AT\",\n",
      "  ROUND(\"ANNUAL_INCOME\", 0) AS \"ANNUAL_INCOME\",\n",
      "  (\n",
      "    \"TOTAL_ORDERS\" / \"TENURE_MONTHS\"\n",
      "  ) AS \"AVERAGE_ORDER_PER_MONTH\",\n",
      "  DATEDIFF(DAY, \"LAST_PURCHASE_DATE\", CURRENT_DATE) AS \"DAYS_SINCE_LAST_PURCHASE\",\n",
      "  DATEDIFF(DAY, \"SIGNUP_DATE\", CURRENT_DATE) AS \"DAYS_SINCE_SIGNUP\",\n",
      "  (\n",
      "    30 / \"PURCHASE_FREQUENCY\"\n",
      "  ) AS \"EXPECTED_DAYS_BETWEEN_PURCHASES\",\n",
      "  ROUND(\n",
      "    (\n",
      "      DATEDIFF(DAY, \"LAST_PURCHASE_DATE\", CURRENT_DATE) - (\n",
      "        30 / \"PURCHASE_FREQUENCY\"\n",
      "      )\n",
      "    ),\n",
      "    0\n",
      "  ) AS \"DAYS_SINCE_EXPECTED_LAST_PURCHASE_DATE\"\n",
      "FROM cte_2 AS cte_2\n"
     ]
    }
   ],
   "source": [
    "# Format and print the SQL for the Snowpark Dataframe\n",
    "ppd_sql = formatSQL(preprocessed_data.queries['queries'][0], True)\n",
    "print(os.linesep.join(ppd_sql.split(os.linesep)[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Preprocessing FeatureView from Preprocess Dataframe (SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature View : FV_PREPROCESS_V_1 already created\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"NAME\"         |\"VERSION\"  |\"DATABASE_NAME\"         |\"SCHEMA_NAME\"   |\"CREATED_ON\"                |\"OWNER\"       |\"DESC\"                       |\"ENTITIES\"    |\"REFRESH_FREQ\"  |\"REFRESH_MODE\"  |\"SCHEDULING_STATE\"  |\"WAREHOUSE\"  |\"CLUSTER_BY\"                            |\"ONLINE_CONFIG\"                                |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|FV_PREPROCESS  |V_1        |RETAIL_REGRESSION_DEMO  |_FEATURE_STORE  |2025-12-19 03:31:12.305000  |ACCOUNTADMIN  |Customer Modelling Features  |[             |1 hour          |FULL            |ACTIVE              |KOGAN_WH     |[\"CUSTOMER_ID\", \"BEHAVIOR_UPDATED_AT\"]  |{\"enable\": false, \"target_lag\": \"10 seconds\"}  |\n",
      "|               |           |                        |                |                            |              |                             |  \"CUSTOMER\"  |                |                |                    |             |                                        |                                               |\n",
      "|               |           |                        |                |                            |              |                             |]             |                |                |                    |             |                                        |                                               |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define descriptions for the FeatureView's Features.  These will be added as comments to the database object\n",
    "preprocess_features_desc = {  \n",
    "   \"AVERAGE_ORDER_PER_MONTH\":\"Average number of orders per month\",\n",
    "   \"DAYS_SINCE_LAST_PURCHASE\":\"Days since last purchase\",\n",
    "   \"DAYS_SINCE_SIGNUP\":\"Days since signup\",\n",
    "   \"EXPECTED_DAYS_BETWEEN_PURCHASES\":\"Expected days between purchases\",\n",
    "   \"DAYS_SINCE_EXPECTED_LAST_PURCHASE_DATE\":\"Days since expected last purchase date from LAST_PURCHASE_DATE\"\n",
    "}\n",
    "\n",
    "ppd_fv_name    = \"FV_PREPROCESS\"\n",
    "ppd_fv_version = \"V_1\"\n",
    "\n",
    "try:\n",
    "   # If FeatureView already exists just return the reference to it\n",
    "   fv_uc01_preprocess = fs.get_feature_view(name=ppd_fv_name,version=ppd_fv_version)\n",
    "except:\n",
    "   # Create the FeatureView instance\n",
    "   fv_uc01_preprocess_instance = FeatureView(\n",
    "      name=ppd_fv_name, \n",
    "      entities=[customer_entity], \n",
    "      feature_df=preprocessed_data,      # <- We can use the snowpark dataframe as-is from our Python\n",
    "      # feature_df=preprocessed_data.queries['queries'][0],    # <- Or we can use SQL, in this case linted from the dataframe generated SQL to make more human readable\n",
    "      timestamp_col=\"BEHAVIOR_UPDATED_AT\",\n",
    "      refresh_freq=\"60 minute\",            # <- specifying optional refresh_freq creates FeatureView as Dynamic Table, else created as View.\n",
    "      desc=\"Customer Modelling Features\").attach_feature_desc(preprocess_features_desc)\n",
    "\n",
    "   # Register the FeatureView instance.  Creates  object in Snowflake\n",
    "   fv_uc01_preprocess = fs.register_feature_view(\n",
    "      feature_view=fv_uc01_preprocess_instance, \n",
    "      version=ppd_fv_version, \n",
    "      block=True,     # whether function call blocks until initial data is available\n",
    "      overwrite=False # whether to replace existing feature view with same name/version\n",
    "   )\n",
    "   print(f\"Feature View : {ppd_fv_name}_{ppd_fv_version} created\")   \n",
    "else:\n",
    "   print(f\"Feature View : {ppd_fv_name}_{ppd_fv_version} already created\")\n",
    "finally:\n",
    "   fs.list_feature_views().show(20)\n",
    "spine = fv_uc01_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# You can also use the following to retrieve a Feature View instance for use within Python\n",
    "FV_UC01_PREPROCESS_V_1 = fs.get_feature_view(ppd_fv_name, 'V_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"AGE\"  |\"GENDER\"  |\"STATE\"  |\"LOYALTY_TIER\"  |\"TENURE_MONTHS\"  |\"SIGNUP_DATE\"  |\"CUSTOMER_CREATED_AT\"       |\"AVG_ORDER_VALUE\"  |\"PURCHASE_FREQUENCY\"  |\"RETURN_RATE\"  |\"LIFETIME_VALUE\"  |\"LAST_PURCHASE_DATE\"  |\"TOTAL_ORDERS\"  |\"BEHAVIOR_UPDATED_AT\"       |\"ANNUAL_INCOME\"  |\"AVERAGE_ORDER_PER_MONTH\"  |\"DAYS_SINCE_LAST_PURCHASE\"  |\"DAYS_SINCE_SIGNUP\"  |\"EXPECTED_DAYS_BETWEEN_PURCHASES\"  |\"DAYS_SINCE_EXPECTED_LAST_PURCHASE_DATE\"  |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|10000          |25     |female    |SA       |low             |7                |2025-05-19     |2025-12-19 03:18:43.893000  |174.47             |1.55                  |4.00           |2351.30           |2025-11-24            |11              |2025-12-02 05:51:10         |87366            |1.571429                   |35                          |224                  |19.354839                          |16                                        |\n",
      "|9999           |25     |female    |QLD      |low             |1                |2025-11-19     |2025-12-19 03:18:43.893000  |198.69             |2.64                  |7.00           |828.91            |2025-11-26            |3               |2025-12-12 12:57:53         |93459            |3.000000                   |33                          |40                   |11.363636                          |22                                        |\n",
      "|9998           |36     |female    |NSW      |medium          |22               |2024-02-19     |2025-12-19 03:18:43.893000  |189.61             |2.66                  |9.00           |13385.16          |2025-12-02            |59              |2025-12-17 19:53:33         |38061            |2.681818                   |27                          |679                  |11.278195                          |16                                        |\n",
      "|9997           |30     |female    |NSW      |medium          |35               |2023-01-19     |2025-12-19 03:18:43.893000  |72.14              |2.63                  |6.00           |8056.36           |2025-11-30            |92              |2025-12-18 03:18:45.088000  |25692            |2.628571                   |29                          |1075                 |11.406844                          |18                                        |\n",
      "|9996           |20     |male      |NT       |low             |51               |2021-09-19     |2025-12-19 03:18:43.893000  |171.37             |1.87                  |9.00           |17690.00          |2025-11-28            |95              |2025-12-18 03:18:45.088000  |36831            |1.862745                   |31                          |1562                 |16.042781                          |15                                        |\n",
      "|9995           |44     |male      |QLD      |medium          |10               |2025-02-19     |2025-12-19 03:18:43.893000  |144.78             |2.55                  |9.00           |4999.93           |2025-11-25            |26              |2025-12-18 03:18:45.088000  |68880            |2.600000                   |34                          |313                  |11.764706                          |22                                        |\n",
      "|9994           |20     |female    |QLD      |low             |59               |2021-01-19     |2025-12-19 03:18:43.893000  |132.78             |3.01                  |13.00          |23582.37          |2025-11-30            |178             |2025-12-18 03:18:45.088000  |28913            |3.016949                   |29                          |1805                 |9.966777                           |19                                        |\n",
      "|9993           |34     |female    |NT       |high            |48               |2021-12-19     |2025-12-19 03:18:43.893000  |98.98              |4.44                  |12.00          |26066.96          |2025-12-11            |213             |2025-12-15 10:34:43         |29892            |4.437500                   |18                          |1471                 |6.756757                           |11                                        |\n",
      "|9992           |18     |female    |NSW      |medium          |71               |2020-01-19     |2025-12-19 03:18:43.893000  |167.34             |3.57                  |17.00          |44970.39          |2025-12-01            |253             |2025-12-18 03:18:45.088000  |31688            |3.563380                   |28                          |2171                 |8.403361                           |20                                        |\n",
      "|9991           |34     |male      |NSW      |medium          |16               |2024-08-19     |2025-12-19 03:18:43.893000  |112.88             |3.38                  |13.00          |7749.95           |2025-11-29            |54              |2025-12-02 22:10:58         |79380            |3.375000                   |30                          |497                  |8.875740                           |21                                        |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can look at the FeatureView's contents with\n",
    "FV_UC01_PREPROCESS_V_1.feature_df.sort(F.col(\"CUSTOMER_ID\"), ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data Dataset from FeatureView_Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"ASOF_DATE\"  |\"COL_1\"  |\n",
      "-----------------------------------------\n",
      "|1              |2025-12-30   |values1  |\n",
      "|2              |2025-12-30   |values1  |\n",
      "|3              |2025-12-30   |values1  |\n",
      "|4              |2025-12-30   |values1  |\n",
      "|5              |2025-12-30   |values1  |\n",
      "-----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Spine\n",
    "spine_sdf = get_spine_df(spine)\n",
    "spine_sdf.sort('CUSTOMER_ID').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.useful_fns import dataset_check_and_update\n",
    "def generate_training_df(spine_sdf, feature_view, feature_store):\n",
    "    dataset_name = 'TRAINING_DATASET'\n",
    "    schema_name = feature_store.list_feature_views().to_pandas()['SCHEMA_NAME'][0]\n",
    "\n",
    "    dataset_version = dataset_check_and_update(session, dataset_name, schema_name= schema_name)\n",
    "    # Generate_Dataset\n",
    "    training_dataset = feature_store.generate_dataset( \n",
    "        name = dataset_name,\n",
    "        version = dataset_version,\n",
    "        spine_df = spine_sdf, \n",
    "        features = [feature_view], \n",
    "        spine_timestamp_col = 'ASOF_DATE'\n",
    "        )                                     \n",
    "    # Create a snowpark dataframe reference from the Dataset\n",
    "    training_dataset_sdf = training_dataset.read.to_snowpark_dataframe()\n",
    "    \n",
    "    return training_dataset_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V_1', 'V_2', 'V_3'] V_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
      "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"ASOF_DATE\"  |\"COL_1\"  |\"AGE\"  |\"GENDER\"  |\"STATE\"  |\"LOYALTY_TIER\"  |\"TENURE_MONTHS\"  |\"SIGNUP_DATE\"  |\"CUSTOMER_CREATED_AT\"       |\"AVG_ORDER_VALUE\"   |\"PURCHASE_FREQUENCY\"  |\"RETURN_RATE\"  |\"LIFETIME_VALUE\"    |\"LAST_PURCHASE_DATE\"  |\"TOTAL_ORDERS\"  |\"ANNUAL_INCOME\"  |\"AVERAGE_ORDER_PER_MONTH\"  |\"DAYS_SINCE_LAST_PURCHASE\"  |\"DAYS_SINCE_SIGNUP\"  |\"EXPECTED_DAYS_BETWEEN_PURCHASES\"  |\"DAYS_SINCE_EXPECTED_LAST_PURCHASE_DATE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|1              |2025-12-30   |values1  |36     |female    |WA       |medium          |6                |2025-06-19     |2025-12-19 03:18:43.893000  |177.22000122070312  |4.070000171661377     |16.0           |4696.9599609375     |2025-12-03            |24              |66091            |4.0                        |16                          |183                  |7.371006965637207                  |9                                         |\n",
      "|2              |2025-12-30   |values1  |32     |female    |NSW      |low             |5                |2025-07-19     |2025-12-19 03:18:43.893000  |109.45999908447266  |1.0                   |7.0            |201.86000061035156  |2025-11-05            |5               |47309            |1.0                        |44                          |153                  |30.0                               |14                                        |\n",
      "|3              |2025-12-30   |values1  |18     |male      |NSW      |low             |29               |2023-07-19     |2025-12-19 03:18:43.893000  |68.95999908447266   |1.440000057220459     |16.0           |2849.530029296875   |2025-11-24            |42              |54797            |1.4482760429382324         |25                          |884                  |20.833332061767578                 |4                                         |\n",
      "|4              |2025-12-30   |values1  |35     |female    |NSW      |low             |22               |2024-02-19     |2025-12-19 03:18:43.893000  |150.57000732421875  |1.5800000429153442    |7.0            |5408.43994140625    |2025-11-22            |35              |27852            |1.5909090042114258         |27                          |669                  |18.987342834472656                 |8                                         |\n",
      "|5              |2025-12-30   |values1  |28     |male      |VIC      |low             |48               |2021-12-19     |2025-12-19 03:18:43.893000  |76.04000091552734   |1.6699999570846558    |4.0            |7551.0              |2025-11-27            |80              |60224            |1.6666669845581055         |22                          |1461                 |17.96407127380371                  |4                                         |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate_Dataset\n",
    "training_dataset_sdf_v1 = generate_training_df(spine_sdf, fv_uc01_preprocess, feature_store=fs)\n",
    "# Display some sample data\n",
    "training_dataset_sdf_v1.sort('CUSTOMER_ID').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>ASOF_DATE</th>\n",
       "      <th>COL_1</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>STATE</th>\n",
       "      <th>LOYALTY_TIER</th>\n",
       "      <th>TENURE_MONTHS</th>\n",
       "      <th>SIGNUP_DATE</th>\n",
       "      <th>CUSTOMER_CREATED_AT</th>\n",
       "      <th>...</th>\n",
       "      <th>RETURN_RATE</th>\n",
       "      <th>LIFETIME_VALUE</th>\n",
       "      <th>LAST_PURCHASE_DATE</th>\n",
       "      <th>TOTAL_ORDERS</th>\n",
       "      <th>ANNUAL_INCOME</th>\n",
       "      <th>AVERAGE_ORDER_PER_MONTH</th>\n",
       "      <th>DAYS_SINCE_LAST_PURCHASE</th>\n",
       "      <th>DAYS_SINCE_SIGNUP</th>\n",
       "      <th>EXPECTED_DAYS_BETWEEN_PURCHASES</th>\n",
       "      <th>DAYS_SINCE_EXPECTED_LAST_PURCHASE_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3441</td>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>values1</td>\n",
       "      <td>31</td>\n",
       "      <td>female</td>\n",
       "      <td>SA</td>\n",
       "      <td>medium</td>\n",
       "      <td>21</td>\n",
       "      <td>2024-03-19</td>\n",
       "      <td>2025-12-19 03:18:43.893</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14172.320312</td>\n",
       "      <td>2025-12-03</td>\n",
       "      <td>64</td>\n",
       "      <td>88795</td>\n",
       "      <td>3.047619</td>\n",
       "      <td>16</td>\n",
       "      <td>640</td>\n",
       "      <td>9.900990</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3648</td>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>values1</td>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>WA</td>\n",
       "      <td>low</td>\n",
       "      <td>24</td>\n",
       "      <td>2023-12-19</td>\n",
       "      <td>2025-12-19 03:18:43.893</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7436.750000</td>\n",
       "      <td>2025-12-08</td>\n",
       "      <td>67</td>\n",
       "      <td>87415</td>\n",
       "      <td>2.791667</td>\n",
       "      <td>11</td>\n",
       "      <td>731</td>\n",
       "      <td>10.791367</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6015</td>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>values1</td>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>NSW</td>\n",
       "      <td>medium</td>\n",
       "      <td>8</td>\n",
       "      <td>2025-04-19</td>\n",
       "      <td>2025-12-19 03:18:43.893</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2771.850098</td>\n",
       "      <td>2025-12-04</td>\n",
       "      <td>27</td>\n",
       "      <td>22167</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>15</td>\n",
       "      <td>244</td>\n",
       "      <td>8.928571</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6035</td>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>values1</td>\n",
       "      <td>53</td>\n",
       "      <td>male</td>\n",
       "      <td>TAS</td>\n",
       "      <td>medium</td>\n",
       "      <td>17</td>\n",
       "      <td>2024-07-19</td>\n",
       "      <td>2025-12-19 03:18:43.893</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7231.109863</td>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>65</td>\n",
       "      <td>24284</td>\n",
       "      <td>3.823529</td>\n",
       "      <td>22</td>\n",
       "      <td>518</td>\n",
       "      <td>7.812500</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9284</td>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>values1</td>\n",
       "      <td>34</td>\n",
       "      <td>female</td>\n",
       "      <td>SA</td>\n",
       "      <td>low</td>\n",
       "      <td>11</td>\n",
       "      <td>2025-01-19</td>\n",
       "      <td>2025-12-19 03:18:43.893</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4291.930176</td>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>21</td>\n",
       "      <td>60054</td>\n",
       "      <td>1.909091</td>\n",
       "      <td>18</td>\n",
       "      <td>334</td>\n",
       "      <td>15.384615</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_ID   ASOF_DATE    COL_1  AGE  GENDER STATE LOYALTY_TIER  \\\n",
       "0         3441  2025-12-30  values1   31  female    SA       medium   \n",
       "1         3648  2025-12-30  values1   18    male    WA          low   \n",
       "2         6015  2025-12-30  values1   18    male   NSW       medium   \n",
       "3         6035  2025-12-30  values1   53    male   TAS       medium   \n",
       "4         9284  2025-12-30  values1   34  female    SA          low   \n",
       "\n",
       "   TENURE_MONTHS SIGNUP_DATE     CUSTOMER_CREATED_AT  ...  RETURN_RATE  \\\n",
       "0             21  2024-03-19 2025-12-19 03:18:43.893  ...         11.0   \n",
       "1             24  2023-12-19 2025-12-19 03:18:43.893  ...         18.0   \n",
       "2              8  2025-04-19 2025-12-19 03:18:43.893  ...         11.0   \n",
       "3             17  2024-07-19 2025-12-19 03:18:43.893  ...         12.0   \n",
       "4             11  2025-01-19 2025-12-19 03:18:43.893  ...         13.0   \n",
       "\n",
       "   LIFETIME_VALUE  LAST_PURCHASE_DATE  TOTAL_ORDERS ANNUAL_INCOME  \\\n",
       "0    14172.320312          2025-12-03            64         88795   \n",
       "1     7436.750000          2025-12-08            67         87415   \n",
       "2     2771.850098          2025-12-04            27         22167   \n",
       "3     7231.109863          2025-11-27            65         24284   \n",
       "4     4291.930176          2025-12-01            21         60054   \n",
       "\n",
       "   AVERAGE_ORDER_PER_MONTH  DAYS_SINCE_LAST_PURCHASE  DAYS_SINCE_SIGNUP  \\\n",
       "0                 3.047619                        16                640   \n",
       "1                 2.791667                        11                731   \n",
       "2                 3.375000                        15                244   \n",
       "3                 3.823529                        22                518   \n",
       "4                 1.909091                        18                334   \n",
       "\n",
       "   EXPECTED_DAYS_BETWEEN_PURCHASES  DAYS_SINCE_EXPECTED_LAST_PURCHASE_DATE  \n",
       "0                         9.900990                                       6  \n",
       "1                        10.791367                                       0  \n",
       "2                         8.928571                                       6  \n",
       "3                         7.812500                                      14  \n",
       "4                        15.384615                                       3  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_sdf_v1.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python\n",
    "from time import perf_counter\n",
    "\n",
    "# ML\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Snowpark\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "from snowflake.ml.registry import Registry as ModelRegistry\n",
    "from snowflake.snowpark import Session, Row\n",
    "from snowflake.ml.dataset import Dataset\n",
    "from snowflake.ml.dataset import load_dataset\n",
    "from snowflake.ml.experiment import ExperimentTracking\n",
    "from snowflake.ml.experiment.callback.xgboost import SnowflakeXgboostCallback\n",
    "from snowflake.ml.model.model_signature import infer_signature\n",
    "from snowflake.snowpark.context import get_active_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def create_data_connector(session, dataset_name) -> DataConnector:\n",
    "    \"\"\"Load data from Snowflake DataSet\"\"\"\n",
    "    ds = Dataset.load(\n",
    "        session=session, \n",
    "        name=dataset_name\n",
    "    )\n",
    "    ds_latest_version = str(ds.list_versions()[-1])\n",
    "    ds_df = load_dataset(\n",
    "        session, \n",
    "        dataset_name, \n",
    "        ds_latest_version\n",
    "    )\n",
    "    return DataConnector.from_dataset(ds_df)\n",
    "\n",
    "\n",
    "def compare_params(input_d, extracted_d):\n",
    "    ignore_keys = ['callbacks'] # Ignore complex objects\n",
    "    mismatches = []\n",
    "    \n",
    "    for key, val in input_d.items():\n",
    "        if key in ignore_keys: continue\n",
    "            \n",
    "        # Check if key exists in extraction\n",
    "        if key not in extracted_d:\n",
    "            mismatches.append(f\"Missing key: {key}\")\n",
    "            continue\n",
    "            \n",
    "        ex_val = extracted_d[key]\n",
    "        \n",
    "        # Handle Float vs Int (63 vs 63.0) and NaNs\n",
    "        if isinstance(val, (int, float)) and isinstance(ex_val, (int, float)):\n",
    "            # Check for NaN in both (NaN != NaN in Python, so we must handle explicitly)\n",
    "            if pd.isna(val) and pd.isna(ex_val):\n",
    "                continue\n",
    "            if not math.isclose(val, ex_val):\n",
    "                mismatches.append(f\"{key}: {val} (Input) != {ex_val} (Row)\")\n",
    "        \n",
    "        # Standard comparison for strings/others\n",
    "        elif val != ex_val:\n",
    "            mismatches.append(f\"{key}: {val} != {ex_val}\")\n",
    "            \n",
    "    return mismatches\n",
    "\n",
    "def generate_train_val_set(dataframe: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Generate train and validation dataset\"\"\"\n",
    "    # Split data\n",
    "    X = dataframe[['RETURN_RATIO', 'FREQUENCY']]\n",
    "    y = dataframe[\"RETURN_ROW_PRICE\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Splitted data\")\n",
    "\n",
    "    # Combine features and target for each split\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "    val_df = pd.concat([X_test, y_test], axis=1)\n",
    "    return train_df, val_df\n",
    "\n",
    "def build_pipeline(**model_params) -> Pipeline:\n",
    "    \"\"\"Create pipeline with preprocessors and model\"\"\"\n",
    "    # Define column types\n",
    "    feature_cols = ['RETURN_RATIO', 'FREQUENCY'] \n",
    "\n",
    "    # Create preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('NUM', MinMaxScaler(), feature_cols)\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "    )\n",
    "\n",
    "    model = xgb.XGBRegressor(**(model_params))\n",
    "\n",
    "    return Pipeline([(\"preprocessor\", preprocessor), (\"regressor\", model)])\n",
    "\n",
    "\n",
    "def evaluate_model(model: Pipeline, X_test: pd.DataFrame, y_test: pd.DataFrame):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"mean_absolute_error\": mean_absolute_error(y_test, y_pred),\n",
    "        \"mean_absolute_percentage_error\": mean_absolute_percentage_error(y_test, y_pred),\n",
    "        \"r2_score\": r2_score(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train():\n",
    "    from snowflake.ml.modeling import tune\n",
    "    from snowflake.ml.modeling.tune.search import RandomSearch, BayesOpt\n",
    "    session = get_active_session()\n",
    "    # Get tuner context\n",
    "    tuner_context = tune.get_tuner_context()\n",
    "    params = tuner_context.get_hyper_params()\n",
    "    dm = tuner_context.get_dataset_map()\n",
    "    model_name = params.pop(\"model_name\")\n",
    "    mr_schema_name = params.pop(\"mr_schema_name\")\n",
    "    experiment_name = params.pop(\"experiment_name\")\n",
    "    \n",
    "    # Initialize experiment tracking for this trial\n",
    "    exp = ExperimentTracking(session=session, schema_name=mr_schema_name)\n",
    "    exp.set_experiment(experiment_name)\n",
    "\n",
    "    run = exp.start_run()\n",
    "    print(\"OG NAME!!!!!\")\n",
    "    print(run.name)\n",
    "    print(\"++++++++++++++\")\n",
    "\n",
    "    # with exp.start_run():\n",
    "    # Load data\n",
    "    train_data = dm[\"train\"].to_pandas()\n",
    "    val_data = dm[\"val\"].to_pandas()\n",
    "\n",
    "    # Separate features and target\n",
    "    X_train = train_data.drop('RETURN_ROW_PRICE', axis=1)\n",
    "    y_train = train_data['RETURN_ROW_PRICE']\n",
    "    X_val = val_data.drop('RETURN_ROW_PRICE', axis=1)\n",
    "    y_val = val_data['RETURN_ROW_PRICE']\n",
    "\n",
    "    # Train model\n",
    "    sig = infer_signature(X_train, y_train)\n",
    "    callback = SnowflakeXgboostCallback(\n",
    "        exp, model_name=\"name\", model_signature=sig\n",
    "    )\n",
    "    params['callbacks'] = [callback]\n",
    "\n",
    "    model = build_pipeline(\n",
    "        model_params=params\n",
    "    )\n",
    "    # Log model parameters with the log_param(...) or log_params(...) methods\n",
    "    exp.log_params(params)\n",
    "\n",
    "    print(\"Training model...\", end=\"\")\n",
    "    start = perf_counter()\n",
    "    model.fit(X_train, y_train)\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\", end=\"\")\n",
    "    start = perf_counter()\n",
    "    metrics = evaluate_model(\n",
    "        model,\n",
    "        X_val,\n",
    "        y_val,\n",
    "    )\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    # Log model metrics with the log_metric(...) or log_metrics(...) methods\n",
    "    exp.log_metrics(metrics)\n",
    "\n",
    "    # Report to HPO framework (optimize on validation F1)\n",
    "    tuner_context.report(\n",
    "        metrics=metrics, \n",
    "        model=model\n",
    "    )\n",
    "    return {\n",
    "        \"run_name\": run.name, \n",
    "        \"params\": params,\n",
    "        \"mean_absolute_error\": metrics['mean_absolute_error'],\n",
    "        \"mean_absolute_percentage_error\": metrics['mean_absolute_percentage_error'],\n",
    "        \"r2_score\": metrics['r2_score'],\n",
    "        \"model\": model,\n",
    "        \"X_train\": X_train,\n",
    "        \"metrics\": metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_remote(\n",
    "        source_dataset: str, \n",
    "        model_name: str, \n",
    "        mr_schema_name: str,\n",
    "        experiment_name: str\n",
    "    ):\n",
    "    from snowflake.ml.modeling import tune\n",
    "    from snowflake.ml.modeling.tune.search import RandomSearch, BayesOpt\n",
    "\n",
    "    # Retrieve session from SPCS service context\n",
    "    session = Session.builder.getOrCreate()\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\", end=\"\", flush=True)\n",
    "    start = perf_counter()\n",
    "    dc = create_data_connector(session, dataset_name=source_dataset)\n",
    "    df = dc.to_pandas()\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\" done! Loaded {len(df)} rows, elapsed={elapsed:.3f}s\")\n",
    "\n",
    "    print(f\"Building train/val data\")\n",
    "    train_df, val_df = generate_train_val_set(df)\n",
    "\n",
    "    X = train_df[['RETURN_RATIO', 'FREQUENCY']]\n",
    "    y = train_df[\"RETURN_ROW_PRICE\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    # Create DataConnectors\n",
    "    dataset_map = {\n",
    "        \"train\": DataConnector.from_dataframe(session.create_dataframe(train_df)),\n",
    "        \"val\": DataConnector.from_dataframe(session.create_dataframe(val_df)),\n",
    "    }\n",
    "\n",
    "    # Define search space for XGBoost\n",
    "    search_space = {\n",
    "        'mr_schema_name': mr_schema_name,\n",
    "        'model_name': model_name,\n",
    "        'experiment_name': experiment_name,\n",
    "        'n_estimators': tune.randint(50, 200),\n",
    "        'random_state': 42,\n",
    "    }\n",
    "\n",
    "    # Configure tuner\n",
    "    tuner_config = tune.TunerConfig(\n",
    "        metric='mean_absolute_percentage_error',\n",
    "        mode='min',\n",
    "        search_alg=RandomSearch(),\n",
    "        num_trials=2,\n",
    "    )\n",
    "\n",
    "    # Create tuner\n",
    "    tuner = tune.Tuner(\n",
    "        train_func=train,\n",
    "        search_space=search_space, \n",
    "        tuner_config=tuner_config\n",
    "    )\n",
    "\n",
    "    print(f\"HPO starting\")\n",
    "    results = tuner.run(dataset_map=dataset_map)\n",
    "\n",
    "    best_config = results.best_result[0] if isinstance(results.best_result, list) else results.best_result\n",
    "    best_model = results.best_model[0] if isinstance(results.best_model, list) else results.best_model\n",
    "    best_config_record = best_config.to_dict(orient='records')[0]\n",
    "    best_config_dict = {\n",
    "        str(k).removeprefix('config/'): v \n",
    "        for k, v in best_config_record.items() \n",
    "        if k.startswith('config/')\n",
    "    }\n",
    "    results_df: pd.DataFrame = results.results\n",
    "    exp = ExperimentTracking(session=session, schema_name=mr_schema_name)\n",
    "    exp.set_experiment(experiment_name)\n",
    "    param_cols = [c for c in results_df.columns if str(c).startswith('params/')]\n",
    "\n",
    "    for index, row in results_df.iterrows():\n",
    "        run_name = row['run_name']\n",
    "        exp.start_run(run_name)\n",
    "\n",
    "        # run = runs.run_name\n",
    "        metrics = {\n",
    "            \"mean_absolute_error\": row['metrics/mean_absolute_error'],\n",
    "            \"mean_absolute_percentage_error\": row['metrics/mean_absolute_percentage_error'],\n",
    "            \"r2_score\": row['metrics/r2_score'],\n",
    "        }\n",
    "        params_series = row[param_cols]\n",
    "        params_dict = {\n",
    "            str(k).removeprefix('params/'): v \n",
    "            for k, v in params_series.items()\n",
    "        }\n",
    "        diffs = compare_params(best_config_dict, params_dict)\n",
    "\n",
    "        if not diffs:\n",
    "            # Save model to registry\n",
    "            print(\"Logging model to Model Registry...\", end=\"\")\n",
    "            exp.log_model(\n",
    "                model=best_model, \n",
    "                model_name=model_name, \n",
    "                metrics=metrics,\n",
    "                sample_input_data=X_train,\n",
    "                conda_dependencies=[\"xgboost\"],\n",
    "            ) # type: ignore\n",
    "            \n",
    "        exp.end_run(row['run_name'])\n",
    "    return {\n",
    "        \"results\": results.results,\n",
    "        \"best_config\": best_config,\n",
    "        \"best_model\": best_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'tune' from 'snowflake.ml.modeling' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_job \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_remote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTPCXAI_SF0001_QUICKSTART_INC._TRAINING_FEATURE_STORE.UC01_TRAINING\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMODEL_1.UC01_SNOWFLAKEML_RF_REGRESSOR_MODEL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmr_schema_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMODEL_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMY_EXPERIMENT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 7\u001b[0m, in \u001b[0;36mtrain_remote\u001b[0;34m(source_dataset, model_name, mr_schema_name, experiment_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_remote\u001b[39m(\n\u001b[1;32m      2\u001b[0m         source_dataset: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m      3\u001b[0m         model_name: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m      4\u001b[0m         mr_schema_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      5\u001b[0m         experiment_name: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      6\u001b[0m     ):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tune\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomSearch, BayesOpt\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Retrieve session from SPCS service context\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'tune' from 'snowflake.ml.modeling' (unknown location)"
     ]
    }
   ],
   "source": [
    "train_job = train_remote(\n",
    "    source_dataset=\"TPCXAI_SF0001_QUICKSTART_INC._TRAINING_FEATURE_STORE.UC01_TRAINING\",\n",
    "    model_name = \"MODEL_1.UC01_SNOWFLAKEML_RF_REGRESSOR_MODEL\",\n",
    "    mr_schema_name = \"MODEL_1\",\n",
    "    experiment_name=\"MY_EXPERIMENT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAN UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "formatted_time = datetime.now(ZoneInfo(\"Australia/Melbourne\")).strftime(\"%A, %B %d, %Y %I:%M:%S %p %Z\")\n",
    "\n",
    "print(f\"The last run time in Melbourne is: {formatted_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflake_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
