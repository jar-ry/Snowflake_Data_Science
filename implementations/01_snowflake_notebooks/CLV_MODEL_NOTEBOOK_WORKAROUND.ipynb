{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "name": "Header"
   },
   "source": [
    "# End to End CLV Model Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "collapsed": false,
    "name": "Description"
   },
   "source": [
    "# Getting Started with Snowflake Feature Store\n",
    "We will use the Use-Case to show how Snowflake Feature Store (and Model Registry) can be used to maintain & store features, retrieve them for training and perform micro-batch inference.\n",
    "\n",
    "In the development (TRAINING) enviroment we will \n",
    "- create FeatureViews in the Feature Store that maintain the required customer-behaviour features.\n",
    "- use these Features to train a model, and save the model in the Snowflake model-registry.\n",
    "- plot the clusters for the trained model to visually verify. \n",
    "\n",
    "In the production (SERVING) environment we will\n",
    "- re-create the FeatureViews on production data\n",
    "- generate an Inference FeatureView that uses the saved model to perform incremental inference\n",
    "\n",
    "# Feature Engineering & Model Training\n",
    "#### Notebook Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "language": "python",
    "metadata": {},
    "name": "imports"
   },
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import os\n",
    "import json\n",
    "import timeit\n",
    "\n",
    "# SNOWFLAKE\n",
    "# Snowpark\n",
    "from snowflake.snowpark import Session, DataFrame, Window, WindowSpec\n",
    "\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "\n",
    "# Snowflake Feature Store\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureView,\n",
    "    Entity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913754ab-7834-424e-818f-fe2162573c0e",
   "metadata": {
    "collapsed": false,
    "name": "cell42"
   },
   "source": [
    "### Set up helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0126317-a6e9-45b1-b91e-b5240a545432",
   "metadata": {
    "language": "python",
    "name": "pip_install_sqlglot"
   },
   "outputs": [],
   "source": [
    "!pip install sqlglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df8b6a-1a19-43af-a7e5-9eeef8f2fe49",
   "metadata": {
    "language": "python",
    "name": "helper_functions"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from snowflake.snowpark import Session, DataFrame, Window, WindowSpec\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import numpy as np\n",
    "from snowflake.snowpark.version import VERSION\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.ml.dataset import Dataset\n",
    "from snowflake.ml._internal.exceptions import (\n",
    "    dataset_errors\n",
    ")\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import ast\n",
    "def check_and_update(df, model_name):\n",
    "    \"\"\"\n",
    "    Check and update the version numbering scheme for Model Registry \n",
    "    to get the next version number for a model.\n",
    "    df         : dataframe from show_models\n",
    "    model_name : model-name to acquire next version for\n",
    "    \"\"\"\n",
    "    if \".\" in model_name:\n",
    "        model_name = model_name.split(\".\")[-1]\n",
    "    if df.empty:\n",
    "        return \"V_1\"\n",
    "    elif df[df[\"name\"] == model_name].empty:\n",
    "        return \"V_1\"\n",
    "    else:\n",
    "        # Increment model_version if df is not a pandas Series\n",
    "        # The result is a Series where each element is a list (e.g., [ \"V_1\", \"V_2\" ])\n",
    "        list_of_lists = df[\"versions\"].apply(ast.literal_eval)\n",
    "\n",
    "        # Step 2: Flatten the list of lists into a single list\n",
    "        all_versions = [version for sublist in list_of_lists for version in sublist]\n",
    "\n",
    "        # Extract only the number part from each string\n",
    "        nums = [int(v.rsplit(\"_\", 1)[-1]) for v in all_versions]\n",
    "        nums.sort()\n",
    "        \n",
    "        # Work with the highest number\n",
    "        last_num = nums[-1]\n",
    "        new_last_value = f\"V_{last_num + 1}\"\n",
    "        return new_last_value\n",
    "    \n",
    "def dataset_check_and_update(session, dataset_name, schema_name = None):\n",
    "    \"\"\"\n",
    "    Check and update the version numbering scheme for Dataset\n",
    "    to get the next version number for a dataset.\n",
    "    session         : current session\n",
    "    dataset_name : dataset_name to acquire next version for\n",
    "    \"\"\"\n",
    "    if schema_name is None:\n",
    "        schema_name = session.get_current_schema()\n",
    "    full_name = session.get_current_database() + \".\" + schema_name + \".\" + dataset_name\n",
    "\n",
    "    try:\n",
    "        ds = Dataset.load(session=session, name=full_name)\n",
    "        versions = ds.list_versions()\n",
    "    except dataset_errors.DatasetNotExistError:\n",
    "        return \"V_1\"\n",
    "\n",
    "    if len(versions) == 0:\n",
    "        return \"V_1\"\n",
    "    else:\n",
    "        # Extract only the number part from each string\n",
    "        nums = [int(v.rsplit(\"_\", 1)[-1]) for v in versions]\n",
    "        nums.sort()\n",
    "        \n",
    "        # Work with the highest number\n",
    "        last_num = nums[-1]\n",
    "        new_last_value = f\"V_{last_num + 1}\"\n",
    "        return new_last_value \n",
    "\n",
    "def get_latest(df, model_name):\n",
    "    \"\"\"\n",
    "    Check and update the version numbering scheme for Model Registry \n",
    "    to get the next version number for a model.\n",
    "    df         : dataframe from show_models\n",
    "    model_name : model-name to acquire next version for\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return \"V_1\"\n",
    "    elif df[df[\"name\"] == model_name].empty:\n",
    "        return \"V_1\"\n",
    "    else:\n",
    "        # 1. Parse string to list\n",
    "        raw_versions = ast.literal_eval(df[\"versions\"][0])\n",
    "\n",
    "        # 2. Sort based on the numeric suffix only\n",
    "        # This removes the prefix context during comparison so \"v_10\" > \"v_2\"\n",
    "        lst = sorted(raw_versions, key=lambda x: int(x.rsplit(\"_\", 1)[-1]))\n",
    "\n",
    "        # 3. Extract the highest value\n",
    "        last_value = lst[-1]\n",
    "        prefix, num = last_value.rsplit(\"_\", 1)\n",
    "\n",
    "        # 4. Increment the number\n",
    "        new_last_value = f\"{prefix}_{int(num) + 1}\"\n",
    "        return new_last_value \n",
    "\n",
    "import sqlglot\n",
    "import sqlglot.optimizer.optimizer\n",
    "def formatSQL (query_in:str, subq_to_cte = False):\n",
    "    \"\"\"\n",
    "    Prettify the given raw SQL statement to nest/indent appropriately.\n",
    "    Optionally replace subqueries with CTEs.\n",
    "    query_in    : The raw SQL query to be prettified\n",
    "    subq_to_cte : When TRUE convert nested sub-queries to CTEs\n",
    "    \"\"\"\n",
    "    expression = sqlglot.parse_one(query_in)\n",
    "    if subq_to_cte:\n",
    "        query_in = sqlglot.optimizer.optimizer.eliminate_subqueries(expression).sql()\n",
    "    return sqlglot.transpile(query_in, read='snowflake', pretty=True)[0]\n",
    "\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml._internal.utils import identifier  \n",
    "def create_ModelRegistry(session, database, mr_schema = 'MODEL_1'):\n",
    "    \"\"\"\n",
    "    Create Snowflake Model Registry if not exists and return as reference.\n",
    "    session   : Snowpark session\n",
    "    database  : Database to use for Model Registry\n",
    "    mr_schema : Schema name to create/use for Model Registry\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cs = session.get_current_schema()\n",
    "        session.sql(f''' create schema {mr_schema} ''').collect()\n",
    "        mr = Registry(session=session, database_name= database, schema_name=mr_schema)\n",
    "        session.sql(f''' use schema {cs}''').collect()\n",
    "    except:\n",
    "        print(f\"Model Registry ({mr_schema}) already exists\")   \n",
    "        mr = Registry(session=session, database_name= database, schema_name=mr_schema)\n",
    "    else:\n",
    "        print(f\"Model Registry ({mr_schema}) created\")\n",
    "\n",
    "    return mr   \n",
    "\n",
    "from snowflake.ml.feature_store import (FeatureStore,CreationMode) \n",
    "def create_FeatureStore(session, database, fs_schema, warehouse):\n",
    "    \"\"\"\n",
    "    Create Snowflake Feature Store if not exists and return reference\n",
    "    session   : Snowpark session\n",
    "    database  : Database to use for Feature Store\n",
    "    fs_schema : Schema name to ceate/use to check for Feature Store\n",
    "    warehouse : Warehouse to use as default for Feature Store\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        fs = FeatureStore(session, database, fs_schema, warehouse, creation_mode=CreationMode.FAIL_IF_NOT_EXIST)\n",
    "        print(f\"Feature Store ({fs_schema}) already exists\") \n",
    "    except:\n",
    "        print(f\"Feature Store ({fs_schema}) created\")   \n",
    "        fs = FeatureStore(session, database, fs_schema, warehouse, creation_mode=CreationMode.CREATE_IF_NOT_EXIST)\n",
    "\n",
    "    return fs\n",
    "\n",
    "def get_spine_df(dataframe):\n",
    "    asof_date = datetime.now() \n",
    "    spine_sdf =  dataframe.feature_df.group_by('CUSTOMER_ID').agg( F.lit(asof_date.strftime('%Y-%m-%d')).as_('ASOF_DATE'))#.limit(10)\n",
    "    spine_sdf = spine_sdf.with_column(\"col_1\", F.lit(\"values1\"))\n",
    "    return spine_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "collapsed": false,
    "name": "Setup_connection_markdown"
   },
   "source": [
    "### Setup Snowflake connection and database parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "language": "python",
    "metadata": {},
    "name": "Setup_connection_code"
   },
   "outputs": [],
   "source": [
    "session = get_active_session()\n",
    "role_name = session.get_current_role()\n",
    "database_name = session.get_current_database().strip('\"')\n",
    "schema_name = \"DS\"\n",
    "warehouse_name = session.get_current_warehouse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "collapsed": false,
    "name": "model_dev_markdown"
   },
   "source": [
    "## MODEL DEVELOPMENT\n",
    "* Create Snowflake Model-Registry\n",
    "* Create Snowflake Feature-Store\n",
    "* Establish and Create CUSTOMER Entity in the development Snowflake FeatureStore\n",
    "* Create Source Data references and perform basic data-cleansing\n",
    "* Create & Run Preprocessing Function to create features\n",
    "* Create FeatureView_Preprocess from Preprocess Dataframe SQL\n",
    "* Create training data from FeatureView_Preprocess (asof join)\n",
    "* Create & Fit Snowpark-ml pipeline \n",
    "* Save model in Model Registry\n",
    "* 'Verify' and approve model\n",
    "* Create new FeatureView_Model_Inference with Transforms UDF + KMeans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "language": "python",
    "metadata": {},
    "name": "create_mr_fs"
   },
   "outputs": [],
   "source": [
    "# Create/Reference Snowflake Model Registry - Common across Environments\n",
    "mr = create_ModelRegistry(session, database_name, '_MODELLING')\n",
    "\n",
    "# Create/Reference Snowflake Feature Store - Common across Environments\n",
    "fs = create_FeatureStore(session, database_name, '_FEATURE_STORE', warehouse_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "explore_data"
   },
   "outputs": [],
   "source": [
    "cust_tbl = '.'.join([database_name, schema_name,'CUSTOMERS'])\n",
    "cust_sdf = session.table(cust_tbl)\n",
    "print(cust_tbl, cust_sdf.count())\n",
    "cust_sdf.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "collapsed": false,
    "name": "entity_markdown"
   },
   "source": [
    "### CUSTOMER Entity\n",
    "Establish and Create CUSTOMER Entity in Snowflake FeatureStore for this Use-Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "language": "python",
    "metadata": {},
    "name": "create_entity"
   },
   "outputs": [],
   "source": [
    "if \"CUSTOMER\" not in json.loads(fs.list_entities().select(F.to_json(F.array_agg(\"NAME\", True))).collect()[0][0]):\n",
    "    customer_entity = Entity(name=\"CUSTOMER\", join_keys=[\"CUSTOMER_ID\"],desc=\"Primary Key for CUSTOMER ORDER\")\n",
    "    fs.register_entity(customer_entity)\n",
    "else:\n",
    "    customer_entity = fs.get_entity(\"CUSTOMER\")\n",
    "\n",
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "collapsed": false,
    "name": "Load_source_markdown"
   },
   "source": [
    " ### Create & Load Source Data\n",
    " Our Feature engineering pipelines are defined using Snowpark dataframes (or SQL expressions).  In the `QS_feature_engineering_fns.py` file we have created two feature engineering functions to create our pipeline :\n",
    "* __uc01_load_data__(order_data: DataFrame, lineitem_data: DataFrame, order_returns_data: DataFrame) -> DataFrame   \n",
    "* __uc01_pre_process__(data: DataFrame) -> DataFrame\n",
    "\n",
    "`uc01_load_data`, takes the source tables, as dataframe objects, and joins them together, performing some data-cleansing by replacing NA's with default values. It returns a dataframe as it's output.\n",
    "\n",
    "`uc01_pre_process`, takes the dataframe output from `uc01_load_data`  and performs aggregation on it to derive some features that will be used in our segmentation model.  It returns a dataframe as output, which we will use to provide the feature-pipeline definition within our FeatureView.\n",
    "\n",
    "In this way we can build up a complex pipeline step-by-step and use it to derive a FeatureView, that will be maintained as a pipeline in Snowflake.\n",
    "\n",
    "We will import the functions, and create dataframes from them using the dataframes we created earlier pointing to the tables in our TRAINING (Development) schema.  We will use the last dataframe we create at the end of the pipeline as our input to the FeatureView.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "language": "python",
    "name": "feature_engineering_funcs"
   },
   "outputs": [],
   "source": [
    "# Feature Engineering Functions\n",
    "def uc01_load_data(customer_data: DataFrame, behavior_data: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Merges order, linetime and order_returns data and replaces Nulls/None with appropriate default values.\n",
    "    customer_data      : A dataframe referencing the \"CUSTOMER\" table in the relevant schema\n",
    "    behavior_data      : A dataframe referencing the \"PURCHASE_BEHAVIOR\" table in the relevant schema\n",
    "\n",
    "    Returns            : Merged/cleansed dataframe with required columns\n",
    "    \"\"\"\n",
    "    # Merge two dataframes\n",
    "    raw_data =  customer_data.join(\n",
    "            behavior_data,\n",
    "            (customer_data[\"CUSTOMER_ID\"] == behavior_data[\"CUSTOMER_ID\"]),\n",
    "            \"left\"\n",
    "        )\\\n",
    "        .rename(\n",
    "            {\n",
    "                customer_data[\"UPDATED_AT\"]: \"CUSTOMER_UPDATED_AT\",\n",
    "                customer_data[\"CUSTOMER_ID\"]: \"CUSTOMER_ID\",\n",
    "                behavior_data[\"UPDATED_AT\"]: \"BEHAVIOR_UPDATED_AT\"\n",
    "            })\n",
    "\n",
    "    return raw_data[[\n",
    "        \"CUSTOMER_ID\",\n",
    "        \"AGE\", \n",
    "        \"GENDER\",\n",
    "        \"STATE\",\n",
    "        \"ANNUAL_INCOME\", \n",
    "        \"LOYALTY_TIER\", \n",
    "        \"TENURE_MONTHS\", \n",
    "        \"SIGNUP_DATE\", \n",
    "        \"CUSTOMER_UPDATED_AT\", \n",
    "        \"AVG_ORDER_VALUE\", \n",
    "        \"PURCHASE_FREQUENCY\", \n",
    "        \"RETURN_RATE\", \n",
    "        \"LIFETIME_VALUE\", \n",
    "        \"LAST_PURCHASE_DATE\", \n",
    "        \"TOTAL_ORDERS\", \n",
    "        \"BEHAVIOR_UPDATED_AT\"\n",
    "    ]]\n",
    "\n",
    "def uc01_pre_process(data: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Performs model-agnostic Feature-Engineering to prepare data for model input for Customer Entity level features\n",
    "    data         : A dataframe containing the merged/cleansed data from CUSTOMERS and PURCHASE_BEHAVIOR tables\n",
    "    result       : Customer level model input features\n",
    "    \"\"\"\n",
    "    # Round annual income to no decimals\n",
    "    data = data\\\n",
    "        .with_column(\"ANNUAL_INCOME\", F.round(F.col(\"ANNUAL_INCOME\"), 0))\n",
    "    \n",
    "    # Generate new features from existing columns\n",
    "    data = data.with_columns([\n",
    "        \"AVERAGE_ORDER_PER_MONTH\",\n",
    "        \"DAYS_SINCE_LAST_PURCHASE\", \n",
    "        \"DAYS_SINCE_SIGNUP\",\n",
    "        \"EXPECTED_DAYS_BETWEEN_PURCHASES\",\n",
    "        \"DAYS_SINCE_EXPECTED_LAST_PURCHASE_DATE\",\n",
    "    ], [\n",
    "        F.col(\"TOTAL_ORDERS\") / F.col(\"TENURE_MONTHS\"),\n",
    "        F.datediff(\"day\", F.col(\"LAST_PURCHASE_DATE\"), F.col(\"BEHAVIOR_UPDATED_AT\")),\n",
    "        F.datediff(\"day\", F.col(\"SIGNUP_DATE\"), F.col(\"BEHAVIOR_UPDATED_AT\")),\n",
    "        F.lit(30) / F.col(\"PURCHASE_FREQUENCY\"),\n",
    "        F.round(F.datediff(\"day\", F.col(\"LAST_PURCHASE_DATE\"), F.col(\"BEHAVIOR_UPDATED_AT\")) - F.lit((F.lit(30) / F.col(\"PURCHASE_FREQUENCY\"))),0)\n",
    "    ])\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "read_data"
   },
   "outputs": [],
   "source": [
    "# Tables\n",
    "cust_tbl                    = '.'.join([database_name, schema_name,'CUSTOMERS'])\n",
    "behavior_tbl                = '.'.join([database_name, schema_name,'PURCHASE_BEHAVIOR'])\n",
    "\n",
    "# Snowpark Dataframe\n",
    "cust_sdf              = session.table(cust_tbl)\n",
    "behavior_tbl          = session.table(behavior_tbl)\n",
    "\n",
    "# Row Counts\n",
    "print(f'''\\nTABLE ROW_COUNTS IN {schema_name}''')\n",
    "print(cust_tbl, cust_sdf.count())\n",
    "print(behavior_tbl, behavior_tbl.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000018",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "metadata": {},
    "name": "load_data"
   },
   "outputs": [],
   "source": [
    "# Call load data feature engineering function\n",
    "raw_data = uc01_load_data(cust_sdf, behavior_tbl)\n",
    "\n",
    "# Format and print the SQL for the Snowpark Dataframe\n",
    "rd_sql = formatSQL(raw_data.queries['queries'][0], True)\n",
    "print(os.linesep.join(rd_sql.split(os.linesep)[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000019",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "metadata": {},
    "name": "show_raw_data"
   },
   "outputs": [],
   "source": [
    "raw_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000020",
   "metadata": {
    "collapsed": false,
    "name": "preprocessing_markdown"
   },
   "source": [
    "### Create & Run Preprocessing Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000021",
   "metadata": {
    "language": "python",
    "metadata": {},
    "name": "preprocessing_feature_engineering"
   },
   "outputs": [],
   "source": [
    "preprocessed_data = uc01_pre_process(raw_data)\n",
    "preprocessed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000023",
   "metadata": {
    "language": "python",
    "metadata": {},
    "name": "preprocessing_SQL_query"
   },
   "outputs": [],
   "source": [
    "# Format and print the SQL for the Snowpark Dataframe\n",
    "ppd_sql = formatSQL(preprocessed_data.queries['queries'][0], True)\n",
    "print(os.linesep.join(ppd_sql.split(os.linesep)[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000024",
   "metadata": {
    "collapsed": false,
    "name": "Create_fv_from_df_markdown"
   },
   "source": [
    "### Create Preprocessing FeatureView from Preprocess Dataframe (SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000025",
   "metadata": {
    "language": "python",
    "metadata": {},
    "name": "Create_dv_from_df"
   },
   "outputs": [],
   "source": [
    "# Define descriptions for the FeatureView's Features.  These will be added as comments to the database object\n",
    "preprocess_features_desc = {  \n",
    "   \"AVERAGE_ORDER_PER_MONTH\":\"Average number of orders per month\",\n",
    "   \"DAYS_SINCE_LAST_PURCHASE\":\"Days since last purchase\",\n",
    "   \"DAYS_SINCE_SIGNUP\":\"Days since signup\",\n",
    "   \"EXPECTED_DAYS_BETWEEN_PURCHASES\":\"Expected days between purchases\",\n",
    "   \"DAYS_SINCE_EXPECTED_LAST_PURCHASE_DATE\":\"Days since expected last purchase date from LAST_PURCHASE_DATE\"\n",
    "}\n",
    "\n",
    "ppd_fv_name    = \"FV_PREPROCESS\"\n",
    "ppd_fv_version = \"V_1\"\n",
    "\n",
    "\n",
    "# Create the FeatureView instance\n",
    "fv_uc01_preprocess_instance = FeatureView(\n",
    "  name=ppd_fv_name, \n",
    "  entities=[customer_entity], \n",
    "  feature_df=preprocessed_data,      # <- We can use the snowpark dataframe as-is from our Python\n",
    "  # feature_df=preprocessed_data.queries['queries'][0],    # <- Or we can use SQL, in this case linted from the dataframe generated SQL to make more human readable\n",
    "  timestamp_col=\"BEHAVIOR_UPDATED_AT\",\n",
    "  refresh_freq=\"60 minute\",            # <- specifying optional refresh_freq creates FeatureView as Dynamic Table, else created as View.\n",
    "  desc=\"Customer Modelling Features\").attach_feature_desc(preprocess_features_desc)\n",
    "\n",
    "# Register the FeatureView instance.  Creates  object in Snowflake\n",
    "fv_uc01_preprocess = fs.register_feature_view(\n",
    "  feature_view=fv_uc01_preprocess_instance, \n",
    "  version=ppd_fv_version, \n",
    "  block=True,     # whether function call blocks until initial data is available\n",
    "  overwrite=True # whether to replace existing feature view with same name/version\n",
    ")\n",
    "\n",
    "spine = fv_uc01_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000026",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "metadata": {},
    "name": "get_fv"
   },
   "outputs": [],
   "source": [
    "# You can also use the following to retrieve a Feature View instance for use within Python\n",
    "FV_UC01_PREPROCESS_V_1 = fs.get_feature_view(ppd_fv_name, 'V_1')\n",
    "\n",
    "# We can look at the FeatureView's contents with\n",
    "FV_UC01_PREPROCESS_V_1.feature_df.sort(F.col(\"CUSTOMER_ID\"), ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000028",
   "metadata": {
    "collapsed": false,
    "name": "Create_train_dataset"
   },
   "source": [
    "### Create training data Dataset from FeatureView_Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000029",
   "metadata": {
    "language": "python",
    "metadata": {},
    "name": "Generate_spine"
   },
   "outputs": [],
   "source": [
    "# Create Spine\n",
    "spine_sdf = get_spine_df(spine)\n",
    "spine_sdf.sort('CUSTOMER_ID').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000030",
   "metadata": {
    "language": "python",
    "name": "define_gen_train_df_func"
   },
   "outputs": [],
   "source": [
    "def generate_training_df(spine_sdf, feature_view, feature_store):\n",
    "    dataset_name = 'TRAINING_DATASET'\n",
    "    schema_name = feature_store.list_feature_views().to_pandas()['SCHEMA_NAME'][0]\n",
    "\n",
    "    dataset_version = dataset_check_and_update(session, dataset_name, schema_name= schema_name)\n",
    "    # Generate_Dataset\n",
    "    training_dataset = feature_store.generate_dataset( \n",
    "        name = dataset_name,\n",
    "        version = dataset_version,\n",
    "        spine_df = spine_sdf, \n",
    "        features = [feature_view], \n",
    "        spine_timestamp_col = 'ASOF_DATE'\n",
    "        )                                     \n",
    "    # Create a snowpark dataframe reference from the Dataset\n",
    "    training_dataset_sdf = training_dataset.read.to_snowpark_dataframe()\n",
    "    \n",
    "    return training_dataset_sdf, schema_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000031",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "metadata": {},
    "name": "gen_train_df"
   },
   "outputs": [],
   "source": [
    "# Generate_Dataset\n",
    "training_dataset_sdf_v1, fv_schema_name = generate_training_df(spine_sdf, fv_uc01_preprocess, feature_store=fs)\n",
    "# Display some sample data\n",
    "training_dataset_sdf_v1.sort('CUSTOMER_ID').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000032",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Pandas_Head"
   },
   "outputs": [],
   "source": [
    "training_dataset_sdf_v1.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000034",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Def_Train"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# ML\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "\n",
    "# Snowpark\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "from snowflake.ml.registry import Registry as ModelRegistry\n",
    "from snowflake.snowpark import Session, Row\n",
    "from snowflake.ml.dataset import Dataset\n",
    "from snowflake.ml.dataset import load_dataset\n",
    "from snowflake.ml.experiment import ExperimentTracking\n",
    "from snowflake.ml.experiment.callback.xgboost import SnowflakeXgboostCallback\n",
    "from snowflake.ml.model.model_signature import infer_signature\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "\n",
    "def create_data_connector(session, dataset_name) -> DataConnector:\n",
    "    \"\"\"Load data from Snowflake DataSet\"\"\"\n",
    "    ds = Dataset.load(\n",
    "        session=session, \n",
    "        name=dataset_name\n",
    "    )\n",
    "    ds_latest_version = str(ds.list_versions()[-1])\n",
    "    ds_df = load_dataset(\n",
    "        session, \n",
    "        dataset_name, \n",
    "        ds_latest_version\n",
    "    )\n",
    "\n",
    "    return DataConnector.from_dataset(ds_df)\n",
    "\n",
    "\n",
    "def compare_params(input_d, extracted_d):\n",
    "    ignore_keys = ['callbacks'] # Ignore complex objects\n",
    "    mismatches = []\n",
    "    \n",
    "    for key, val in input_d.items():\n",
    "        if key in ignore_keys: continue\n",
    "            \n",
    "        # Check if key exists in extraction\n",
    "        if key not in extracted_d:\n",
    "            mismatches.append(f\"Missing key: {key}\")\n",
    "            continue\n",
    "            \n",
    "        ex_val = extracted_d[key]\n",
    "        \n",
    "        # Handle Float vs Int (63 vs 63.0) and NaNs\n",
    "        if isinstance(val, (int, float)) and isinstance(ex_val, (int, float)):\n",
    "            # Check for NaN in both (NaN != NaN in Python, so we must handle explicitly)\n",
    "            if pd.isna(val) and pd.isna(ex_val):\n",
    "                continue\n",
    "            if not math.isclose(val, ex_val):\n",
    "                mismatches.append(f\"{key}: {val} (Input) != {ex_val} (Row)\")\n",
    "        \n",
    "        # Standard comparison for strings/others\n",
    "        elif val != ex_val:\n",
    "            mismatches.append(f\"{key}: {val} != {ex_val}\")\n",
    "            \n",
    "    return mismatches\n",
    "\n",
    "def generate_train_val_set(dataframe: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Generate train and validation dataset\"\"\"\n",
    "    # Split data\n",
    "    X = dataframe[[\n",
    "        'AGE', \n",
    "        'GENDER',\n",
    "        'LOYALTY_TIER', \n",
    "        'TENURE_MONTHS', \n",
    "        'AVG_ORDER_VALUE', \n",
    "        'PURCHASE_FREQUENCY', \n",
    "        'RETURN_RATE', \n",
    "        'TOTAL_ORDERS',\n",
    "        'ANNUAL_INCOME', \n",
    "        'AVERAGE_ORDER_PER_MONTH', \n",
    "        'DAYS_SINCE_LAST_PURCHASE', \n",
    "        'DAYS_SINCE_SIGNUP', \n",
    "        'EXPECTED_DAYS_BETWEEN_PURCHASES',\n",
    "        'DAYS_SINCE_EXPECTED_LAST_PURCHASE_DATE'\n",
    "    ]]\n",
    "    y = dataframe[\"LIFETIME_VALUE\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Splitted data\")\n",
    "\n",
    "    # Combine features and target for each split\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "    val_df = pd.concat([X_test, y_test], axis=1)\n",
    "    return train_df, val_df\n",
    "\n",
    "def build_pipeline(**model_params) -> Pipeline:\n",
    "    \"\"\"Create pipeline with preprocessors and model\"\"\"\n",
    "    # Define column types\n",
    "    ordinal_feature_cols = ['LOYALTY_TIER'] \n",
    "    categorical_feature_cols = ['GENDER'] \n",
    "    numerical_feature_cols = [\n",
    "        'AGE',\n",
    "        'TENURE_MONTHS',\n",
    "        'AVG_ORDER_VALUE',\n",
    "        'PURCHASE_FREQUENCY',\n",
    "        'RETURN_RATE',\n",
    "        'TOTAL_ORDERS',\n",
    "        'ANNUAL_INCOME',\n",
    "        'AVERAGE_ORDER_PER_MONTH',\n",
    "        'DAYS_SINCE_LAST_PURCHASE',\n",
    "        'DAYS_SINCE_SIGNUP',\n",
    "        'EXPECTED_DAYS_BETWEEN_PURCHASES',\n",
    "        'DAYS_SINCE_EXPECTED_LAST_PURCHASE_DATE'\n",
    "    ] \n",
    "    passthrough_feature_cols = [] \n",
    "    \n",
    "    # Create preprocessing steps\n",
    "    tier_order = ['low', 'medium', 'high']\n",
    "    explicit_categories = [tier_order]\n",
    "    ordinal_encoder = OrdinalEncoder(categories=explicit_categories, dtype=int)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('NUM', MinMaxScaler(), numerical_feature_cols),\n",
    "            ('CAT', OneHotEncoder(), categorical_feature_cols),\n",
    "            ('ORD', ordinal_encoder, ordinal_feature_cols)\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "    )\n",
    "\n",
    "    model = xgb.XGBRegressor(**(model_params))\n",
    "\n",
    "    return Pipeline([(\"preprocessor\", preprocessor), (\"regressor\", model)])\n",
    "\n",
    "\n",
    "def evaluate_model(model: Pipeline, X_test: pd.DataFrame, y_test: pd.DataFrame):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"mean_absolute_error\": mean_absolute_error(y_test, y_pred),\n",
    "        \"mean_absolute_percentage_error\": mean_absolute_percentage_error(y_test, y_pred),\n",
    "        \"r2_score\": r2_score(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train():\n",
    "    from snowflake.ml.modeling import tune\n",
    "    from snowflake.ml.registry import Registry\n",
    "    from snowflake.ml.model.volatility import Volatility\n",
    "    \n",
    "    session = get_active_session()\n",
    "    # Get tuner context\n",
    "    tuner_context = tune.get_tuner_context()\n",
    "    params = tuner_context.get_hyper_params()\n",
    "    dm = tuner_context.get_dataset_map()\n",
    "    model_name = params.pop(\"model_name\")\n",
    "    mr_schema_name = params.pop(\"mr_schema_name\")\n",
    "    experiment_name = params.pop(\"experiment_name\")\n",
    "    \n",
    "    # Initialize experiment tracking for this trial\n",
    "    exp = ExperimentTracking(session=session, schema_name=mr_schema_name)\n",
    "    exp.set_experiment(experiment_name)\n",
    "    registry = Registry(session=session)\n",
    "\n",
    "    with exp.start_run() as run:\n",
    "        # Load data\n",
    "        train_data = dm[\"train\"].to_pandas()\n",
    "        val_data = dm[\"val\"].to_pandas()\n",
    "    \n",
    "        # Separate features and target\n",
    "        X_train = train_data.drop('LIFETIME_VALUE', axis=1)\n",
    "        y_train = train_data['LIFETIME_VALUE']\n",
    "        X_val = val_data.drop('LIFETIME_VALUE', axis=1)\n",
    "        y_val = val_data['LIFETIME_VALUE']\n",
    "    \n",
    "        # Train model\n",
    "        sig = infer_signature(X_train, y_train)\n",
    "        callback = SnowflakeXgboostCallback(\n",
    "            exp, model_name=model_name, model_signature=sig\n",
    "        )\n",
    "        params['callbacks'] = [callback]\n",
    "    \n",
    "        model = build_pipeline(\n",
    "            model_params=params\n",
    "        )\n",
    "        # Log model parameters with the log_param(...) or log_params(...) methods\n",
    "        exp.log_params(params)\n",
    "    \n",
    "        print(\"Training model...\", end=\"\")\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "        # Evaluate model\n",
    "        print(\"Evaluating model...\", end=\"\")\n",
    "        metrics = evaluate_model(\n",
    "            model,\n",
    "            X_val,\n",
    "            y_val,\n",
    "        )\n",
    "        \n",
    "        print(\"Log metrics...\", end=\"\")    \n",
    "        # Log model metrics with the log_metric(...) or log_metrics(...) methods\n",
    "        exp.log_metrics(metrics)\n",
    "\n",
    "        metrics['run_name'] = run.name\n",
    "\n",
    "        # Log model\n",
    "        exp.log_model(\n",
    "            model=model, \n",
    "            model_name=model_name, \n",
    "            version_name=run.name,\n",
    "            sample_input_data=X_train\n",
    "            # target_platforms=[\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n",
    "            # options={\n",
    "            #     \"volatility\": Volatility.IMMUTABLE,\n",
    "            #     # \"enable_explainability\": True\n",
    "            # }\n",
    "            # target_platforms=[\"WAREHOUSE\",\"SNOWPARK_CONTAINER_SERVICES\"]\n",
    "        )\n",
    "\n",
    "        # Report to HPO framework (optimize on validation F1)\n",
    "        tuner_context.report(\n",
    "            metrics=metrics,\n",
    "            model=model\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000035",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Def_Train_Remote"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling import tune\n",
    "from snowflake.ml.modeling.tune.search import RandomSearch, BayesOpt\n",
    "def train_remote(\n",
    "        source_dataset: str, \n",
    "        model_name: str, \n",
    "        mr_schema_name: str,\n",
    "        experiment_name: str,\n",
    "        session\n",
    "    ):\n",
    "    # Load data\n",
    "    print(\"Loading data...\", end=\"\", flush=True)\n",
    "    dc = create_data_connector(session, dataset_name=source_dataset)\n",
    "    df = dc.to_pandas()\n",
    "\n",
    "    print(f\"Building train/val data\")\n",
    "    train_df, val_df = generate_train_val_set(df)\n",
    "\n",
    "    # Create DataConnectors\n",
    "    dataset_map = {\n",
    "        \"train\": DataConnector.from_dataframe(session.create_dataframe(train_df)),\n",
    "        \"val\": DataConnector.from_dataframe(session.create_dataframe(val_df)),\n",
    "    }\n",
    "\n",
    "    # Define search space for XGBoost (intentionally mixes strong and weak configs)\n",
    "    search_space = {\n",
    "        'mr_schema_name': mr_schema_name,\n",
    "        'model_name': model_name,\n",
    "        'experiment_name': experiment_name,\n",
    "        # model capacity\n",
    "        'max_depth': tune.choice([1, 4, 6, 10]),\n",
    "        # learning rate: very small underfits, very large can explode\n",
    "        'eta': tune.choice([0.01, 0.1, 0.8]),\n",
    "        # boosting rounds: low for underfit, high for stronger models\n",
    "        'n_estimators': tune.choice([10, 150, 500]),\n",
    "        # row/column subsampling to vary bias/variance\n",
    "        'subsample': tune.choice([0.5, 0.7, 1.0]),\n",
    "        # regularization knobs to swing between over/under-fitting\n",
    "        'reg_lambda': tune.choice([0.1, 1, 10]),\n",
    "        'random_state': tune.choice([42]),\n",
    "    }\n",
    "\n",
    "    # Configure tuner (increase trials to see both good and bad configs)\n",
    "    tuner_config = tune.TunerConfig(\n",
    "        metric='mean_absolute_percentage_error',\n",
    "        mode='min',\n",
    "        search_alg=RandomSearch(),\n",
    "        num_trials=10,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Create tuner\n",
    "    tuner = tune.Tuner(\n",
    "        train_func=train,\n",
    "        search_space=search_space, \n",
    "        tuner_config=tuner_config\n",
    "    )\n",
    "\n",
    "    print(f\"HPO starting\")\n",
    "    results = tuner.run(dataset_map=dataset_map)\n",
    "    print(\"HPO DONE\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000036",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Train_Remote"
   },
   "outputs": [],
   "source": [
    "results = train_remote(\n",
    "    source_dataset=database_name+\".\"+fv_schema_name+\".\"+\"TRAINING_DATASET\",\n",
    "    model_name = \"_MODELLING.UC01_SNOWFLAKEML_RF_REGRESSOR_MODEL\",\n",
    "    mr_schema_name = \"_MODELLING\",\n",
    "    experiment_name=\"MY_EXPERIMENT\",\n",
    "    session=session\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be728078-b3ed-49ab-ae77-9d997c05c417",
   "metadata": {
    "collapsed": false,
    "name": "Product_Best_Model"
   },
   "source": [
    "# Productionise Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eed9af-40e2-477d-bc99-87fa62ce0d9f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell53"
   },
   "outputs": [],
   "source": [
    "best_result = results.best_result\n",
    "best_run_name = best_result[best_result['run_name'].notnull()]['run_name'].iloc[0]\n",
    "all_results = results.results\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89124df1-4d68-4b45-a470-25b0b3947b7f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "get_best_result"
   },
   "outputs": [],
   "source": [
    "best_run = all_results[all_results[\"run_name\"] == best_run_name]['run_name'].iloc[0]\n",
    "best_model_name = all_results[all_results[\"run_name\"] == best_run_name]['config/model_name'].iloc[0].split(\".\")[1]\n",
    "\n",
    "# Initialize experiment tracking for this trial\n",
    "model_object = mr.get_model(best_model_name)\n",
    "model_versions = model_object.show_versions()\n",
    "best_version = model_object.version(best_run)\n",
    "best_version_df = model_versions[model_versions['name'] == best_run]\n",
    "best_version_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c016a96-a4f1-4e8c-a67f-9b8a90231eb7",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Setting model for production there are 4 options:\n",
    "# Using the default version\n",
    "model_object.default = best_run\n",
    "\n",
    "# Using aliases\n",
    "try:\n",
    "    best_version.set_alias(\"PROD\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Using tags\n",
    "session.sql(f\"\"\"CREATE TAG {mr._database_name}.{mr._schema_name}.live_model_version;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2c09c-14a8-433f-8db4-33b4da0d66c5",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "model_object.set_tag(f\"\"\"{mr._database_name}.{mr._schema_name}.live_model_version\"\"\", best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91705f-1e50-446e-b9f8-44210455a1bd",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "# Using multiple schemas\n",
    "session.sql(f\"\"\"CREATE OR REPLACE SCHEMA {mr._database_name}.{\"PROD_SCHEMA\"};\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5ed3e-9d4c-4de2-b235-4d21b1f852dc",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "session.sql(f\"\"\"\n",
    "CREATE OR REPLACE MODEL {mr._database_name}.{\"PROD_SCHEMA\"}.{best_model_name}\n",
    "WITH VERSION {best_run}\n",
    "FROM MODEL {mr._database_name}.{mr._schema_name}.{best_model_name}\n",
    "VERSION {best_run};\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e1056-0b08-450b-95ed-35f8b0152e15",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "print(model_object.show_tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930cc414-1e06-47f6-985a-998f8540538e",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "# Initialize experiment tracking for this trial\n",
    "model_object_v2 = mr.get_model(best_model_name)\n",
    "model_versions_v2 = model_object_v2.show_versions()\n",
    "best_version_v2 = model_versions_v2[model_versions_v2['name'] == best_run]\n",
    "best_version_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b79586-88e5-4cad-a4f3-fd504a50d881",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "model_object_v2.version(\"DEFAULT\").create_service(service_name=\"clv_service\",\n",
    "                  service_compute_pool=\"CLV_MODEL_POOL_CPU\",\n",
    "                  ingress_enabled=True,\n",
    "                  gpu_requests=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee328f-c25a-44e8-bf7c-0a54c010a97c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell51"
   },
   "outputs": [],
   "source": [
    "# Get signature of the inference function in Python\n",
    "model_object_v2.version(\"DEFAULT\").show_functions()\n",
    "# Call the function in Python\n",
    "service_prediction = model_object_v2.version(\"DEFAULT\").run(\n",
    "    training_dataset_sdf_v1,\n",
    "    function_name=\"predict\",\n",
    "    service_name=\"clv_service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "def serve(featurevector, km4_purchases) -> DataFrame:\n",
    "    return km4_purchases.run(\n",
    "        featurevector, \n",
    "        function_name=\"predict\",\n",
    "        service_name=\"clv_service\"\n",
    "    )\n",
    "\n",
    "# Test Inference process\n",
    "preprocessed_data_df = fs.get_feature_view(ppd_fv_name, 'V_1').feature_df\n",
    "inference_result_sdf = serve(\n",
    "    preprocessed_data_df, \n",
    "    model_object_v2.version(\"DEFAULT\")\n",
    ").with_column_renamed('\"output_feature_0\"', \"PREDICTION\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "session.use_schema(\"DS\")\n",
    "\n",
    "inference_result_sdf.write.mode(\"overwrite\").save_as_table(\"prediction_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "session.table(\"prediction_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.monitoring.entities.model_monitor_config import ModelMonitorConfig, ModelMonitorSourceConfig\n",
    "\n",
    "source_config = ModelMonitorSourceConfig(\n",
    "    source=\"RETAIL_REGRESSION_DEMO.DS.prediction_table\",\n",
    "    timestamp_column=\"BEHAVIOR_UPDATED_AT\",\n",
    "    id_columns=[\"CUSTOMER_ID\"],\n",
    "    prediction_score_columns=[\"PREDICTION\"],\n",
    "    actual_score_columns=[\"LIFETIME_VALUE\"],\n",
    ")\n",
    "\n",
    "# Set up config for ModelMonitor.\n",
    "model_monitor_config = ModelMonitorConfig(\n",
    "    model_version=model_object_v2.version(\"DEFAULT\"),\n",
    "    model_function_name=\"predict\",\n",
    "    background_compute_warehouse_name=\"COMPUTE_WH\",\n",
    "    refresh_interval=\"1 hour\",\n",
    "    aggregation_window=\"1 day\"\n",
    ")\n",
    "mr.delete_monitor(f\"\"\"{best_model_name}_monitor\"\"\")\n",
    "\n",
    "session.use_schema(\"_MODELLING\")\n",
    "# Add a new ModelMonitor\n",
    "model_monitor = mr.add_monitor(\n",
    "    name=f\"\"\"{best_model_name}_monitor\"\"\", \n",
    "    source_config=source_config,\n",
    "    model_monitor_config=model_monitor_config,\n",
    ")\n",
    "model_monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000037",
   "metadata": {
    "collapsed": false,
    "name": "cell38"
   },
   "source": [
    "## CLEAN UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000038",
   "metadata": {
    "language": "python",
    "metadata": {},
    "name": "cell39"
   },
   "outputs": [],
   "source": [
    "# session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000039",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "formatted_time = datetime.now(ZoneInfo(\"Australia/Melbourne\")).strftime(\"%A, %B %d, %Y %I:%M:%S %p %Z\")\n",
    "\n",
    "print(f\"The last run time in Melbourne is: {formatted_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000040",
   "metadata": {
    "language": "python",
    "name": "cell41"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflake_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "lastEditStatus": {
   "authorEmail": "jarry.chen@snowflake.com",
   "authorId": "6363208213288",
   "authorName": "JARCHEN",
   "lastEditTime": 1768889094794,
   "notebookId": "sqpavyn43vhfjn7jwqxc",
   "sessionId": "accade85-dce8-4c00-8197-3576e7930336"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
